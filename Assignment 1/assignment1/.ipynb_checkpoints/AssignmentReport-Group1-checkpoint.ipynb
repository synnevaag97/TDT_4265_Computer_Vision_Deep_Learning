{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "$C(\\omega) = \\frac{1}{N} \\sum_{n=1}^{N}C^{N}(\\omega) = \\frac{1}{N} \\sum_{n=1}^{N}[-(y^nln(\\hat{y}^n) + (1-y^n)ln(1-\\hat{y}^n))] $\n",
    "\n",
    "The derivative of C is easy to compute when we have found the derivative of $C^n$.\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = \\frac{\\partial C^n(\\omega)}{\\partial \\hat{y}^n}\\frac{\\partial \\hat{y}^n}{\\partial \\omega_i} = [-y^n*\\frac{1}{\\hat{y}^n} \\frac{\\partial \\hat{y}^n}{\\partial \\omega_i} - (1-y^n)\\frac{1}{1-\\hat{y}^n}(-1) \\frac{\\partial \\hat{y}^n}{\\partial \\omega_i}]$\n",
    "\n",
    "We have that $\\hat{y}^n = f(x^n_i)$\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = [-\\frac{y^n}{\\hat{y}^n} \\frac{\\partial f(x^n_i)}{\\partial \\omega_i} + \\frac{1-y^n}{1-\\hat{y}^n}\\frac{\\partial f(x^n_i)}{\\partial \\omega_i}] $\n",
    "\n",
    "The derivative of f with regards to $\\omega$ is: \n",
    "$\\frac{\\partial f^{x^n_i}}{\\partial \\omega_i} = x^n_if(x^n_i)(1-f(x^n_i))$\n",
    "\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = [-\\frac{y^n}{\\hat{y}^n} x^n_if(x^n_i)(1-f(x^n_i)) + \\frac{1-y^n}{1-\\hat{y}^n}x^n_if(x^n_i)(1-f(x^n_i))] $\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = [-y^nx^n_i(1-\\hat{y}^n) + (1-y^n)x^n_i\\hat{y}^n] $\n",
    "\n",
    "This yields the resulting derivative: \n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = (\\hat{y}^n-y^n)x^n_i $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "The cross entropy function: \n",
    "\n",
    "$C(\\omega_{kj}) = \\frac{1}{N} \\sum_{n=1}^{N}C^{N}(\\omega_{kj}) = -\\frac{1}{N} \\sum_{n=1}^{N}\\sum_{k=1}^{K}[y^n_kln(\\hat{y}^n_k)] $\n",
    "\n",
    "We now have a multiclass detector such that the outputs of our neural network and the corresponding targets are also defined by the number of output nodes, K. \n",
    "\n",
    "Again we find the derivative of C by finding the derivative of $C^n$. \n",
    "\n",
    "We have that $\\hat{y}^n_k = \\frac{e^{z_k}}{\\sum_{k'=1}^{K'}e^{z_{k'}}}$\n",
    "\n",
    "\n",
    "$ C^n(\\omega_{kj}) = -\\sum_{k=1}^{K}[y^n_kln(\\frac{e^{z_k}}{\\sum_{k'=1}^{K'}e^{z_{k'}}})] $\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_{kj}} = \\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k}\\frac{\\partial z_k}{\\partial \\omega_{kj}}$\n",
    "\n",
    "The derivative of $z_k$ with respect to $\\omega_{kj}$ is:\n",
    "\n",
    "$\\frac{\\partial z_k}{\\partial \\omega_{kj}} = x $\n",
    "\n",
    "\n",
    "For the derivative of $C^n$ with respect to $z_k$ we have to consider two cases. When k=k' and when k!=k'. First we use the core rule:\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = - \\sum_{k=1}^{K}[y^n_k \\frac{u}*u')$. \n",
    "\n",
    "where $ u = \\frac{e^{z_k}}{\\sum_{k'=1}^{K'}e^{z_{k'}}}$ and $ u' = \\frac{(e^{z_k})'\\sum_{k'=1}^{K'}e^{z_{k'}} - e^{z_k}(\\sum_{k'=1}^{K'}e^{z_{k'}})'}{(\\sum_{k'=1}^{K'}e^{z_{k'}})^2}$\n",
    "\n",
    "Case 1: k$=$k'\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k'}\\frac{\\sum_{k'=1}^{K'}e^{z_{k'}}}{e^{z_k}}[\\frac{e^{z_k}*\\sum_{k'=1}^{K'}e^{z_{k'}} - e^{z_k}(\\sum_{k'=1}^{K'}e^{z_{k'}})'}{(e^{z_{k'}})^2}]$\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k'} \\frac{1}{\\hat{y}^n_k}[\\hat{y}_k - \\hat{y}_k\\hat{y}_{k'}] = y^n_{k'}(1-\\hat{y}_{k'}) $\n",
    "\n",
    "Case 2: k$\\neq$k'\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k'}\\frac{\\sum_{k'=1}^{K'}e^{z_{k'}}}{e^{z_k}}[\\frac{0*\\sum_{k'=1}^{K'}e^{z_{k'}} - e^{z_k}(\\sum_{k'=1}^{K'}e^{z_{k}})'}{(e^{z_{k'}})^2}]$\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k} \\frac{1}{\\hat{y}^n_k}[- \\hat{y}_k\\hat{y}_{k'}] = -y^n_{k}\\hat{y}_{k'} $\n",
    "\n",
    "\n",
    "This gives the final expression: \n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = x[y^n_{k'}(1-\\hat{y}_{k'}) + (-y^n_{k}(\\hat{y}_{k'}))] = x[y^n_{k'}-y^n_{k'}\\hat{y}_{k'} -y^n_{k}\\hat{y}_{k'}] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "The functions was implemented in VSCode successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "The functionality of completing the logistic regression with mini-batch gradient descent for a single layer was implemented. First a forward pass, then the loss was calculated based on the output compared to the expected target output. Then backpropegation was computed and the weights was updated based on the calculated gradient. \n",
    "\n",
    "The resulting loss over training steps is visualized in the figure below. This is before we have implemented early stoppage, therefore the NN train till there are no more samples. \n",
    "\n",
    "![Loss evolving over training steps.](task2b_binary_train_loss.png)\n",
    "\n",
    "The loss starts to stagnate around 600 sample with very small improvement after that. The final loss is around 0.075. \n",
    "\n",
    "During the training the loss have some significant peaks which are large compared to the pattern. This originates from that periodically a very hard test set comes along where it misses completely. This is periodically because all the bad test set come at the same time resulting in a very large peek. If the samples were to be shuffled around in the dataset then the peaks would disappear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "The accuracy function was implemented successfully in the code. The resulting accuracy over training is visualized in the graph below. \n",
    "\n",
    "![Accuracy of fully connected NN over training time.](task2b_binary_train_accuracy.png)\n",
    "\n",
    "The Validation accuracy shows to reach up to 98 percentage which is really good. We also see thath is stagnates up to 600 training steps similar to the loss graph which is resonable. \n",
    "Also it has the same peaks as for the loss graph. \n",
    "\n",
    "The validation set is a set used to test the NN, while the training set is a different dataset used to train and tune the NN. During training the validation set has a larger accuracy than the training set. This is weird as one would expect that the NN would do better on the dataset it is training on. However, this could be completely random and we see that at the end the training set has equal or slitghly better accuracy than the validation set. \n",
    "\n",
    "There is no overfitting for this NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The early stoppage was implemented in the trainer.py. It was confusing how to implement this, however a reasonable method was found. At each validation, 20% of training set, we count if the new accuracy was less than the old accuracy. If this counter reaches 10 then we will stopp early. This means that after 10 checks the accuracy had not improved. This provided a much better plot which stops at a reasonable time. Instead of running for 1600 samples we rather run for closer to 300 steps. We could increase the counter to get a slightly better accuracy. However, the gain in accuracy might not be worth the extra computations. \n",
    "\n",
    "The resulting results are visualized in the graphs below. \n",
    "\n",
    "![Loss after implementing early stoppage](task2c_binary_train_loss.png) ![Accuracy after implementing early stoppage](task2c_binary_train_accuracy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "After implementing a shuffle functionality the resulting progress over the training steps have removed the peaks. This is depicted below. \n",
    "\n",
    "![Shuffle vs no Shuffle](task2e_train_accuracy_shuffle_difference.png)\n",
    "![Loss after implementing shuffling](task2d_binary_train_loss.png)\n",
    "![Accuracy after implementing shuffling](task2d_binary_train_loss.png)\n",
    "\n",
    "The peaks disapear because the bad dataset that came periodically is now random in the datasets. That means that the bad examples are spread out reducing peaks. \n",
    "\n",
    "The first plot visualize accuracy with and without shuffling. However, it looks like there both are shuffled. So there might be something wrong with the code that compute the data that is plotted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a)\n",
    "The functions were implementing the the code sucessfully, with no errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "\n",
    "The code was implemented similarly to Task 2b). The resulting loss over traning steps are visualized in the graph below. \n",
    "\n",
    "![Loss over training steps](task3b_softmax_train_loss.png)\n",
    "\n",
    "The loss has a slightly less decrease over time such that the early stoppage dosen't kick in before 3000 steps. It is also expected for it to take longer to achieve a good loss value compared to the task 2). This is because we now have a classification problem of 10 output nodes instead of a binary problem of 1 output node. \n",
    "\n",
    "\n",
    "There are no peaks in this problem as the shuffling of the dataset is activated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "The function for the accuracy of the NN was implemented successfully. The resulting accuracy over training steps is visualized in graph below. \n",
    "![](task3b_softmax_train_accuracy.png)\n",
    "\n",
    "The accuracy acheive up to 91 percentage. This is considerably lower than 98 percent in task 2b), but also expected as the complexity of the problem is increased. \n",
    "\n",
    "We see again that the validation accuracy is larger for the first half similarly to task 2). At the end the training accuracy is better than the validation which is expected. As it should be better at the dataset it trains on than the dataset it is tested on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "##### Do we observe any overfitting from the graph in problem 3c)?\n",
    "\n",
    "\n",
    "Also here we have no overfitting. Overfitting could be found when the NN does considerably much better on the training set than the validation set. This means that the NN is an expert on the specific dataset it has trained on, but does porely on the dataset it is tested on. We could see some overfitting at the end of the accuracy plot but the early stoppage ensure that overfitting is not a big problem. If we were to remove the early stoppage and let the NN train on the entire dataset overfitting is expected to become a problem. This is shown in the figure below:  \n",
    "\n",
    "![](task3d_softmax_train_accuracy.png)\n",
    "\n",
    "\n",
    "The accuracy improved a lot from the early stoppage, but this is only overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_{i,j}} = \\frac{\\partial C(w)}{\\partial w_{i,j}} + \\lambda\\frac{\\partial R(w)}{\\partial w_{i,j}}$\n",
    "\n",
    "$\\frac{\\partial C(w)}{\\partial w_{i,j}} = -\\frac{1}{N}x_j^n(y_i^n-\\hat{y_i^n})$\n",
    "\n",
    "$\\lambda\\frac{\\partial R(w)}{\\partial w_{i,j}} = \\lambda\\frac{\\partial}{\\partial w}\\frac{1}{2}\\sum_{i,j}w_{i,j}^2 = \\lambda\\frac{1}{2}\\sum_{i,j}\\frac{\\partial}{\\partial w_{i,j}}w_{i,j}^2 = \\lambda w_{i,j}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_{i,j}} = -\\frac{1}{N}x_j^n(y_i^n-\\hat{y_i^n}) + \\lambda w_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "![](task4b_softmax_weight=0.png)\n",
    "![](task4b_softmax_weight=2.png)\n",
    "\n",
    "\n",
    "The images represent the weights for each digits for two Neural Networks trained respectively with $\\lambda$ equals to 0.0 and 2.0.\n",
    "The weights for the model with higher $\\lambda$ are less noisy because of the regularization, it reduces the high difference of intensity between pixel that are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "The reason for this phonomenon is hidden in the semplicity of our model, even without regularization we obtain an optimal result without overfitting, which is one of the major reason for using regularization. Probably the method gives some advantages in situations where the Neural Network is much more complicated and so overfitting could be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4e_l2_reg_norms.png)\n",
    "\n",
    "It is evident that when $\\lambda$ is increased there is a decrease of the $L_2$ norm, which is caused by the regularization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76ae7beaef6cee4f0e88e3e11277b79c96730a54bf10a09e926989ef02c393c5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
