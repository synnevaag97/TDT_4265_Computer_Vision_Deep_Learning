{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "$C(\\omega) = \\frac{1}{N} \\sum_{n=1}^{N}C^{N}(\\omega) = \\frac{1}{N} \\sum_{n=1}^{N}[-(y^nln(\\hat{y}^n) + (1-y^n)ln(1-\\hat{y}^n))] $\n",
    "\n",
    "The derivative of C is easy to compute when we have found the derivative of $C^n$.\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = \\frac{\\partial C^n(\\omega)}{\\partial \\hat{y}^n}\\frac{\\partial \\hat{y}^n}{\\partial \\omega_i} = [-y^n*\\frac{1}{\\hat{y}^n} \\frac{\\partial \\hat{y}^n}{\\partial \\omega_i} - (1-y^n)\\frac{1}{1-\\hat{y}^n}(-1) \\frac{\\partial \\hat{y}^n}{\\partial \\omega_i}]$\n",
    "\n",
    "We have that $\\hat{y}^n = f(x^n_i)$\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = [-\\frac{y^n}{\\hat{y}^n} \\frac{\\partial f(x^n_i)}{\\partial \\omega_i} + \\frac{1-y^n}{1-\\hat{y}^n}\\frac{\\partial f(x^n_i)}{\\partial \\omega_i}] $\n",
    "\n",
    "The derivative of f with regards to $\\omega$ is: \n",
    "$\\frac{\\partial f^{x^n_i}}{\\partial \\omega_i} = x^n_if(x^n_i)(1-f(x^n_i))$\n",
    "\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = [-\\frac{y^n}{\\hat{y}^n} x^n_if(x^n_i)(1-f(x^n_i)) + \\frac{1-y^n}{1-\\hat{y}^n}x^n_if(x^n_i)(1-f(x^n_i))] $\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = [-y^nx^n_i(1-\\hat{y}^n) + (1-y^n)x^n_i\\hat{y}^n] $\n",
    "\n",
    "This yields the resulting derivative: \n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_i} = (\\hat{y}^n-y^n)x^n_i $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "The cross entropy function: \n",
    "\n",
    "$C(\\omega_{kj}) = \\frac{1}{N} \\sum_{n=1}^{N}C^{N}(\\omega_{kj}) = -\\frac{1}{N} \\sum_{n=1}^{N}\\sum_{k=1}^{K}[y^n_kln(\\hat{y}^n_k)] $\n",
    "\n",
    "We now have a multiclass detector such that the outputs of our neural network and the corresponding targets are also defined by the number of output nodes, K. \n",
    "\n",
    "Again we find the derivative of C by finding the derivative of $C^n$. \n",
    "\n",
    "We have that $\\hat{y}^n_k = \\frac{e^{z_k}}{\\sum_{k'=1}^{K'}e^{z_{k'}}}$\n",
    "\n",
    "\n",
    "$ C^n(\\omega_{kj}) = -\\sum_{k=1}^{K}[y^n_kln(\\frac{e^{z_k}}{\\sum_{k'=1}^{K'}e^{z_{k'}}})] $\n",
    "\n",
    "$\\frac{\\partial C^{n}}{\\partial \\omega_{kj}} = \\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k}\\frac{\\partial z_k}{\\partial \\omega_{kj}}$\n",
    "\n",
    "The derivative of $z_k$ with respect to $\\omega_{kj}$ is:\n",
    "\n",
    "$\\frac{\\partial z_k}{\\partial \\omega_{kj}} = x $\n",
    "\n",
    "\n",
    "For the derivative of $C^n$ with respect to $z_k$ we have to consider two cases. When k=k' and when k!=k'. First we use the core rule:\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = - \\sum_{k=1}^{K}[y^n_k \\frac{u}*u')$. \n",
    "\n",
    "where $ u = \\frac{e^{z_k}}{\\sum_{k'=1}^{K'}e^{z_{k'}}}$ and $ u' = \\frac{(e^{z_k})'\\sum_{k'=1}^{K'}e^{z_{k'}} - e^{z_k}(\\sum_{k'=1}^{K'}e^{z_{k'}})'}{(\\sum_{k'=1}^{K'}e^{z_{k'}})^2}$\n",
    "\n",
    "Case 1: k=k'\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k'}\\frac{\\sum_{k'=1}^{K'}e^{z_{k'}}}{e^{z_k}}[\\frac{e^{z_k}*\\sum_{k'=1}^{K'}e^{z_{k'}} - e^{z_k}(\\sum_{k'=1}^{K'}e^{z_{k'}})'}{(e^{z_{k'}})^2}]$\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k'} \\frac{1}{\\hat{y}^n_k}[\\hat{y}_k - \\hat{y}_k\\hat{y}_{k'}] = y^n_{k'}(1-\\hat{y}_{k'}) $\n",
    "\n",
    "Case 2: k!=k'\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k'}\\frac{\\sum_{k'=1}^{K'}e^{z_{k'}}}{e^{z_k}}[\\frac{0*\\sum_{k'=1}^{K'}e^{z_{k'}} - e^{z_k}(\\sum_{k'=1}^{K'}e^{z_{k}})'}{(e^{z_{k'}})^2}]$\n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = y^n_{k} \\frac{1}{\\hat{y}^n_k}[- \\hat{y}_k\\hat{y}_{k'}] = -y^n_{k}\\hat{y}_{k'} $\n",
    "\n",
    "\n",
    "This gives the final experssion: \n",
    "\n",
    "$\\frac{\\partial C^n(\\omega_{kj})}{\\partial z_k} = x[y^n_{k'}(1-\\hat{y}_{k'}) + (-y^n_{k}(\\hat{y}_{k'}))] = x[y^n_{k'}-y^n_{k'}\\hat{y}_{k'} -y^n_{k}\\hat{y}_{k'}] $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "The functions was implemented in VSCode successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "The functionality of completing the logistic regression with mini-batch gradient descent for a single layer was implemented. First a forward pass, then the loss was calculated based on the output compared to the expected target output. Then backpropegation was computed and the weights was updated based on the calculated gradient. \n",
    "\n",
    "The resulting loss over time is visualized in the figure below. This is before we have implemented early stoppage such that the NN train till there are no more sample and also data shuffeling. \n",
    "\n",
    "![Loss evolving over time when training.](task2b_binary_train_loss.png)\n",
    "\n",
    "The loss starts to stagnate around 600 sample with very small improvement after that. During the training the loss have some significant peaks which are large increases compared to the pattern. This can originate from that periodically a very hard test set comes along where it misses completely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "The accuracy function was implemented successfully in the code. The resulting accuracy over training is visualized in the graph below. \n",
    "\n",
    "![Accuracy of fully connected NN over training time.](task2b_binary_train_accuracy.png)\n",
    "\n",
    "\n",
    "\n",
    "The Validation accuracy shows to reach up to 98 percentage which is really good. We also see thath is stagnates up to 600 training steps similar to the loss graph which is resonable. \n",
    "Also it has the same peaks ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The early stoppage was implemented in the trainer.py. It was confusing how to implement this, however a reasonable method was found. At each validation, 20% of training set, we count if the new accuracy was less than the old accuracy. If this counter reaches 10 then we will stopp early. This means that after 10 checks the accuracy had not improved. This provided a much better plot which stops at a reasonable time. Instead of running for 1600 samples we rather run for closer to 300 steps. We could increase the counter to get a slightly better accuracy. However, the gain in accuracy might not be worth the extra computations.  \n",
    "\n",
    "![Loss after implementing early stoppage](task2b_binary_train_loss.png)\n",
    "![Accuracy after implementing early stoppage](task2b_binary_train_accuracy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "After implementing a shuffle functionality the resulting progress over the training steps have few peaks compared to previously. \n",
    "This is because ....\n",
    "\n",
    "![Shuffle vs no Shuffle](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a)\n",
    "The functions were implementing the the code sucessfully, with no errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "\n",
    "The code was implemented similarly to Task 2b). The resulting loss over traning steps are visualized in the graph below. \n",
    "\n",
    "![Loss over training steps](task3b_softmax_train_loss.png)\n",
    "\n",
    "The loss has a slightly less decrease over time such that the early stoppage dosen't kick in before 3000 steps. It is also expected for it to take longer to achieve a good accuracy compared to the task 2). This is because we now have a classification problem of 10 output nodes instead of 1 output node. \n",
    "\n",
    "\n",
    "There are no peaks in this problem as the shuffling of the dataset is activated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "The function for the accuracu of the NN was implemented successfully. The resulting accuracy over training steps is visualized in graph below. \n",
    "![](task3b_softmax_train_accuracy.png)\n",
    "\n",
    "The accuracy acheive up to 91 percentage. This is considerably lower than 98 percent in task 2b), but also expected as the complexity of the problem is increased. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Do we observe any overfitting from the graph in problem 3c)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76ae7beaef6cee4f0e88e3e11277b79c96730a54bf10a09e926989ef02c393c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
